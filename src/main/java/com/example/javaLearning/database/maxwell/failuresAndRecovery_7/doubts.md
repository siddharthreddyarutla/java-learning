## 1ï¸âƒ£ How does Maxwell handle **DDL commands**?

### Short truth

âœ… **Yes, DDLs are captured and published**
âŒ But **they are NOT first-class citizens like DML**

---

### What happens internally

* DDL statements (ALTER, CREATE, DROP, RENAME) **do appear in MySQL binlog**
* Maxwell detects them
* It emits a **DDL event** (schema change event)

Example (conceptual):

```json
{
  "type": "table-alter",
  "database": "users",
  "table": "employee",
  "sql": "ALTER TABLE employee ADD COLUMN age INT"
}
```

ğŸ“Œ Interview-important nuance:

> Maxwell emits DDL **as metadata events**, not as row events.

---

### What Maxwell does NOT do with DDL

âŒ It does NOT:

* enforce schema compatibility
* version schemas
* block incompatible changes
* manage consumer deserialization

Thatâ€™s why:

* Consumers must be **schema-tolerant**
* Or you choose Debezium + Schema Registry

ğŸ“Œ Interview line:

> *â€œMaxwell surfaces DDLs but leaves schema evolution responsibility to consumers.â€*

---

## 2ï¸âƒ£ Can Maxwell break because of queries / DDLs?

### Yes â€” but letâ€™s be precise.

Maxwell can stop if:

* It encounters an **unsupported binlog event**
* Schema change is too complex
* Binlog is corrupted
* Table metadata mismatch

Typical cases:

* Table without primary key (older Maxwell versions)
* Exotic DDL sequences
* Binlog format misconfiguration

---

### What happens when Maxwell breaks?

* Maxwell **stops**
* It does **NOT advance binlog offset**
* No silent data loss

ğŸ“Œ This is critical:

> Maxwell is **fail-fast**, not fail-open.

---

### Do you need to manually update offsets?

âŒ **Almost never in production**

Correct recovery:

1. Fix root cause (schema / config)
2. Restart Maxwell
3. It resumes from **last saved binlog position**

Manual offset manipulation is:

* Dangerous
* Interview red flag
* Used only in emergencies

ğŸ“Œ Interview line:

> *â€œManual offset changes are a last resort and risk data loss.â€*

---

## 3ï¸âƒ£ How does Maxwell publish to Kafka? Where is it configured?

### Architecture truth

Maxwell has a **built-in Kafka producer**.

There is:

* No Kafka Connect
* No external sink

```
Maxwell â†’ KafkaProducer â†’ Kafka
```

---

### Where Kafka is configured (conceptually)

In Maxwell config:

* Kafka brokers
* Producer configs
* Topic naming strategy

Interview-safe explanation:

> â€œKafka configuration is provided directly to Maxwell, which produces events using an embedded Kafka producer.â€

You are **NOT expected** to recite config keys.

---

## 4ï¸âƒ£ How does Maxwell decide **which topic to publish to**?

This is important and often asked.

### Default behavior

ğŸ“Œ **One topic per database**

Example:

```
db.users
db.orders
```

---

### Other supported strategies

* Topic per table

  ```
  users.employee
  users.department
  ```

* Custom routing

* Regex filters

ğŸ“Œ Interview clarity:

> *â€œTopic routing is configurable; the most common strategy is database-level topics.â€*

---

## 5ï¸âƒ£ How does Maxwell handle multiple databases?

* Single Maxwell instance
* Can read **multiple databases**
* Emits events with:

    * database name
    * table name

Kafka topics:

* Either shared
* Or separated per DB

Ordering guarantee:

* Preserved per primary key
* Across DBs ordering is irrelevant

---

## 6ï¸âƒ£ Difference: **Maxwell vs Dynamic Maxwell**

This is a GREAT question â€” many people donâ€™t even know this exists.

### Maxwell (standard)

* Static configuration
* Restart needed for:

    * filters
    * topic changes
    * bootstrap config

---

### Dynamic Maxwell

* Supports **runtime changes**
* Filters, bootstraps, routing updated dynamically
* No restart required

Use cases:

* Large multi-tenant systems
* Frequent table onboarding
* Operational flexibility

ğŸ“Œ Interview positioning:

> *â€œDynamic Maxwell improves operability, not CDC semantics.â€*

CDC guarantees are the **same**.

---

## 7ï¸âƒ£ Final clarity checklist (you should be 100% clear now)

âœ” DML â†’ row events
âœ” DDL â†’ metadata events
âœ” Maxwell does NOT enforce schemas
âœ” Crashes are safe (offsets preserved)
âœ” Kafka producer is embedded
âœ” Topic routing is configurable
âœ” Dynamic Maxwell = operational enhancement

---

## Interview-Perfect Summary (Memorize)

If interviewer compresses all your questions into one, say:

> â€œMaxwell captures both DML and DDL from MySQL binlogs. DMLs become row events, while DDLs are emitted as schema metadata events. It fails fast on unsupported changes without advancing offsets. Kafka publishing is done via an embedded producer with configurable topic routing, commonly one topic per database. Dynamic Maxwell adds runtime configurability without changing CDC guarantees.â€

ğŸ”¥ That answer is **rock solid**.
