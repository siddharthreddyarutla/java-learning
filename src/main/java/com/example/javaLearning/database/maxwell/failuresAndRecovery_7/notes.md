# ðŸ§  STEP 6 â€” Failure Scenarios & Recovery (Interview-Critical)

The hidden interview question here is:

> **â€œWhat breaks in real life, and how does your system recover?â€**

If you answer this cleanly, youâ€™re clearly senior+.

---

## 6.1 First Principle (lock this in)

> **Maxwell is durable, Kafka is durable, consumers are not.**

So failures are expected â€” correctness comes from **replay + idempotency**.

---

## 6.2 Failure #1 â€” Maxwell Process Crashes

### What happens?

* Maxwell may have:

    * already read binlog
    * already sent event to Kafka
    * NOT yet saved binlog position

### Result

* On restart â†’ same binlog events replayed
* Kafka gets **duplicate events**

### Why this is SAFE

* At-least-once delivery
* Ordering preserved
* Consumers must be idempotent

ðŸ“Œ Interview line:

> *â€œMaxwell favors durability over exactly-once guarantees.â€*

---

## 6.3 Failure #2 â€” Kafka Is Down

### What happens?

* Maxwell cannot publish
* It **stops reading further binlogs**
* Binlog position is NOT advanced

### After Kafka recovers

* Maxwell resumes
* No data loss (if binlogs retained)

ðŸ“Œ Interview insight:

> *â€œKafka backpressure naturally pauses CDC consumption.â€*

---

## 6.4 Failure #3 â€” Binlogs Are Purged (MOST DANGEROUS)

### Scenario

* Maxwell is down
* MySQL purges old binlogs
* Maxwell restarts

### Result

âŒ **Data loss â€” unrecoverable**

### Prevention

* Set binlog retention > worst-case downtime
* Monitor Maxwell lag

ðŸ“Œ Interview killer line:

> *â€œBinlog retention defines the recovery window of CDC systems.â€*

---

## 6.5 Failure #4 â€” Consumer Crash

### What happens?

* Kafka replays from last committed offset
* Same events reprocessed

### Correct handling

* Idempotent sink operations
* Dedup for side-effects

ðŸ“Œ Interview truth:

> *â€œConsumer crashes are normal; correctness depends on replay safety.â€*

---

## 6.6 Failure #5 â€” Partial Consumer Success (VERY COMMON)

Example:

* Notification sent
* Consumer crashes
* Offset not committed

Result:

* Notification sent AGAIN

### Correct solution

* Dedup store
* Or Outbox pattern

ðŸ“Œ Interview line:

> *â€œOffsets track reads, not business completion.â€*

---

## 6.7 Failure #6 â€” Schema Change Mid-Stream

### Example

```sql
ALTER TABLE users ADD COLUMN age;
```

### What Maxwell does

* Emits schema change
* Then row events include new column

### Consumer responsibility

* Be forward-compatible
* Ignore unknown fields
* Avoid strict deserialization

ðŸ“Œ Interview insight:

> *â€œCDC consumers must be schema-tolerant.â€*

---

## 6.8 Failure #7 â€” Ordering Breaks (Self-Inflicted)

Ordering breaks ONLY if:
âŒ Kafka key â‰  primary key
âŒ Parallel processing of same key
âŒ Multiple consumers writing same entity

ðŸ“Œ Interview warning:

> Ordering bugs are configuration bugs, not CDC bugs.

---

## 6.9 Recovery Strategies (Interview Checklist)

| Scenario              | Recovery          |
| --------------------- | ----------------- |
| Maxwell crash         | Restart           |
| Consumer bug          | Reset offsets     |
| Data rebuild          | Bootstrap         |
| Downstream corruption | Replay from Kafka |
| Lost binlogs          | Full re-bootstrap |

ðŸ“Œ Interview insight:

> Replayability is the biggest advantage of CDC.

---

## 6.10 One Perfect Interview Answer (Memorize)

If interviewer asks:

> *â€œHow does your system recover from failures?â€*

Say:

> â€œMaxwell provides at-least-once CDC with durable binlog offsets. Kafka allows replay, and consumers are designed to be idempotent. As long as binlogs are retained, we can recover from crashes by replaying events.â€

ðŸ”¥ That answer checks **all boxes**.

---

## Lock-in Mental Model ðŸ§ 

> **CDC systems donâ€™t prevent failures â€” they make failures recoverable.**

If this makes sense, youâ€™re thinking like a production engineer.
